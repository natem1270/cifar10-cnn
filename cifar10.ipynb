{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZWDDMc_tEEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QCfysbzxmuR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42981f86-4450-4865-f950-a615d1e4a9b5"
      },
      "source": [
        "\"\"\"\n",
        "Load & Transform Dataset\n",
        "\"\"\"\n",
        "transforms = transforms.Compose([\n",
        "                  transforms.ToTensor(),\n",
        "                  transforms.RandomHorizontalFlip(),\n",
        "                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))] #normalize with mean & stdev of .5- 3x for each for RGB channels\n",
        ")\n",
        "\n",
        "#load datasets5\n",
        "train_set = datasets.CIFAR10(root=\"../../data/\",\n",
        "                             train=True,\n",
        "                             transform=transforms,\n",
        "                             download=True)\n",
        "test_set = datasets.CIFAR10(root=\"../../data/\",\n",
        "                            train=False,\n",
        "                            transform=transforms,)\n",
        "\n",
        "#data loaders\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
        "                                           batch_size=100,\n",
        "                                           shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
        "                                          shuffle=False,\n",
        "                                          batch_size=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4MsFiix53xn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa683d68-64ae-4643-d789-b4a22eab37f2"
      },
      "source": [
        "#get dataset size\n",
        "first_datapoint = train_set[0]\n",
        "print(\"Dataset Size: {}x{}x{}\".format(*tuple(first_datapoint[0].size())))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Size: 3x32x32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqOYlwlS6BAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Convolutional Network with three convolutional layers and three fully•••••••••••••••••••••••••••••••••••••••••• connected layers\n",
        "\"\"\"\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__() #not sure why all examples use python2 syntax super(ConvNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "                #block1\n",
        "                nn.Conv2d(3, 32, kernel_size=4, padding=1),\n",
        "                nn.BatchNorm2d(32),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=4, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Dropout2d(p=.05)\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=4, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "          nn.Dropout(p=.1),\n",
        "          nn.Linear(128*3*3, 288),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(288, 144),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(p=.1),\n",
        "          nn.Linear(144, num_classes)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byXJTWGa7D11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17b7751f-559a-4fda-a9cd-9e879c052faa"
      },
      "source": [
        "\"\"\"\n",
        "Initialize training parameters\n",
        "\"\"\"\n",
        "num_classes = 10\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "num_epochs = 150"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "778Diout6G5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Setup Model + optimizer and loss function\n",
        "\"\"\"\n",
        "model = ConvNet(num_classes).to(device) #train on selected device\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=.001) #start with learning rate of .001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH6Olik375pB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a6862b53-3c5b-471d-ad9b-92e8235f9cce"
      },
      "source": [
        "\"\"\"\n",
        "Train model\n",
        "\"\"\"\n",
        "\n",
        "losses = []\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images) #feed images into model\n",
        "        loss = criterion(outputs, labels) #calculate loss\n",
        "        optimizer.zero_grad() #reset gradient on each pass\n",
        "        loss.backward() #backpropogation\n",
        "        optimizer.step() #update parameters based on gradient\n",
        "        running_loss += loss.item() * images.size(0) \n",
        "        if (i+1) % 100 == 0:\n",
        "          print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    losses.append(epoch_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/200], Step [100/500], Loss: 2.0080\n",
            "Epoch [1/200], Step [200/500], Loss: 1.7061\n",
            "Epoch [1/200], Step [300/500], Loss: 1.4720\n",
            "Epoch [1/200], Step [400/500], Loss: 1.5576\n",
            "Epoch [1/200], Step [500/500], Loss: 1.2964\n",
            "Epoch [2/200], Step [100/500], Loss: 1.2621\n",
            "Epoch [2/200], Step [200/500], Loss: 1.2774\n",
            "Epoch [2/200], Step [300/500], Loss: 1.0896\n",
            "Epoch [2/200], Step [400/500], Loss: 1.2536\n",
            "Epoch [2/200], Step [500/500], Loss: 0.9286\n",
            "Epoch [3/200], Step [100/500], Loss: 0.8982\n",
            "Epoch [3/200], Step [200/500], Loss: 1.0496\n",
            "Epoch [3/200], Step [300/500], Loss: 0.8939\n",
            "Epoch [3/200], Step [400/500], Loss: 1.0607\n",
            "Epoch [3/200], Step [500/500], Loss: 0.9965\n",
            "Epoch [4/200], Step [100/500], Loss: 0.8393\n",
            "Epoch [4/200], Step [200/500], Loss: 1.1088\n",
            "Epoch [4/200], Step [300/500], Loss: 0.9602\n",
            "Epoch [4/200], Step [400/500], Loss: 0.8441\n",
            "Epoch [4/200], Step [500/500], Loss: 0.8629\n",
            "Epoch [5/200], Step [100/500], Loss: 1.0700\n",
            "Epoch [5/200], Step [200/500], Loss: 0.6036\n",
            "Epoch [5/200], Step [300/500], Loss: 0.8913\n",
            "Epoch [5/200], Step [400/500], Loss: 0.9557\n",
            "Epoch [5/200], Step [500/500], Loss: 0.9556\n",
            "Epoch [6/200], Step [100/500], Loss: 0.6814\n",
            "Epoch [6/200], Step [200/500], Loss: 0.7606\n",
            "Epoch [6/200], Step [300/500], Loss: 0.7829\n",
            "Epoch [6/200], Step [400/500], Loss: 0.5631\n",
            "Epoch [6/200], Step [500/500], Loss: 0.7239\n",
            "Epoch [7/200], Step [100/500], Loss: 0.6912\n",
            "Epoch [7/200], Step [200/500], Loss: 0.6584\n",
            "Epoch [7/200], Step [300/500], Loss: 0.4917\n",
            "Epoch [7/200], Step [400/500], Loss: 0.7859\n",
            "Epoch [7/200], Step [500/500], Loss: 0.7947\n",
            "Epoch [8/200], Step [100/500], Loss: 0.7313\n",
            "Epoch [8/200], Step [200/500], Loss: 0.8522\n",
            "Epoch [8/200], Step [300/500], Loss: 0.6815\n",
            "Epoch [8/200], Step [400/500], Loss: 0.9141\n",
            "Epoch [8/200], Step [500/500], Loss: 1.0249\n",
            "Epoch [9/200], Step [100/500], Loss: 0.5709\n",
            "Epoch [9/200], Step [200/500], Loss: 0.5592\n",
            "Epoch [9/200], Step [300/500], Loss: 0.7150\n",
            "Epoch [9/200], Step [400/500], Loss: 0.7743\n",
            "Epoch [9/200], Step [500/500], Loss: 0.6976\n",
            "Epoch [10/200], Step [100/500], Loss: 0.5329\n",
            "Epoch [10/200], Step [200/500], Loss: 0.6125\n",
            "Epoch [10/200], Step [300/500], Loss: 0.7111\n",
            "Epoch [10/200], Step [400/500], Loss: 0.5311\n",
            "Epoch [10/200], Step [500/500], Loss: 0.5193\n",
            "Epoch [11/200], Step [100/500], Loss: 0.5977\n",
            "Epoch [11/200], Step [200/500], Loss: 0.4471\n",
            "Epoch [11/200], Step [300/500], Loss: 0.7768\n",
            "Epoch [11/200], Step [400/500], Loss: 0.7870\n",
            "Epoch [11/200], Step [500/500], Loss: 0.7495\n",
            "Epoch [12/200], Step [100/500], Loss: 0.4683\n",
            "Epoch [12/200], Step [200/500], Loss: 0.3989\n",
            "Epoch [12/200], Step [300/500], Loss: 0.4793\n",
            "Epoch [12/200], Step [400/500], Loss: 0.8573\n",
            "Epoch [12/200], Step [500/500], Loss: 0.7042\n",
            "Epoch [13/200], Step [100/500], Loss: 0.4281\n",
            "Epoch [13/200], Step [200/500], Loss: 0.4873\n",
            "Epoch [13/200], Step [300/500], Loss: 0.6057\n",
            "Epoch [13/200], Step [400/500], Loss: 0.7495\n",
            "Epoch [13/200], Step [500/500], Loss: 0.5117\n",
            "Epoch [14/200], Step [100/500], Loss: 0.6011\n",
            "Epoch [14/200], Step [200/500], Loss: 0.4228\n",
            "Epoch [14/200], Step [300/500], Loss: 0.4768\n",
            "Epoch [14/200], Step [400/500], Loss: 0.6284\n",
            "Epoch [14/200], Step [500/500], Loss: 0.6728\n",
            "Epoch [15/200], Step [100/500], Loss: 0.4831\n",
            "Epoch [15/200], Step [200/500], Loss: 0.5492\n",
            "Epoch [15/200], Step [300/500], Loss: 0.5252\n",
            "Epoch [15/200], Step [400/500], Loss: 0.5544\n",
            "Epoch [15/200], Step [500/500], Loss: 0.5317\n",
            "Epoch [16/200], Step [100/500], Loss: 0.3516\n",
            "Epoch [16/200], Step [200/500], Loss: 0.6918\n",
            "Epoch [16/200], Step [300/500], Loss: 0.6129\n",
            "Epoch [16/200], Step [400/500], Loss: 0.4097\n",
            "Epoch [16/200], Step [500/500], Loss: 0.5500\n",
            "Epoch [17/200], Step [100/500], Loss: 0.5685\n",
            "Epoch [17/200], Step [200/500], Loss: 0.5512\n",
            "Epoch [17/200], Step [300/500], Loss: 0.4132\n",
            "Epoch [17/200], Step [400/500], Loss: 0.6608\n",
            "Epoch [17/200], Step [500/500], Loss: 0.5447\n",
            "Epoch [18/200], Step [100/500], Loss: 0.4598\n",
            "Epoch [18/200], Step [200/500], Loss: 0.5046\n",
            "Epoch [18/200], Step [300/500], Loss: 0.4568\n",
            "Epoch [18/200], Step [400/500], Loss: 0.3801\n",
            "Epoch [18/200], Step [500/500], Loss: 0.4918\n",
            "Epoch [19/200], Step [100/500], Loss: 0.6205\n",
            "Epoch [19/200], Step [200/500], Loss: 0.6661\n",
            "Epoch [19/200], Step [300/500], Loss: 0.4640\n",
            "Epoch [19/200], Step [400/500], Loss: 0.7971\n",
            "Epoch [19/200], Step [500/500], Loss: 0.5338\n",
            "Epoch [20/200], Step [100/500], Loss: 0.4221\n",
            "Epoch [20/200], Step [200/500], Loss: 0.3758\n",
            "Epoch [20/200], Step [300/500], Loss: 0.6518\n",
            "Epoch [20/200], Step [400/500], Loss: 0.4291\n",
            "Epoch [20/200], Step [500/500], Loss: 0.4657\n",
            "Epoch [21/200], Step [100/500], Loss: 0.4223\n",
            "Epoch [21/200], Step [200/500], Loss: 0.4590\n",
            "Epoch [21/200], Step [300/500], Loss: 0.5451\n",
            "Epoch [21/200], Step [400/500], Loss: 0.4502\n",
            "Epoch [21/200], Step [500/500], Loss: 0.4496\n",
            "Epoch [22/200], Step [100/500], Loss: 0.4967\n",
            "Epoch [22/200], Step [200/500], Loss: 0.4976\n",
            "Epoch [22/200], Step [300/500], Loss: 0.4757\n",
            "Epoch [22/200], Step [400/500], Loss: 0.6094\n",
            "Epoch [22/200], Step [500/500], Loss: 0.4090\n",
            "Epoch [23/200], Step [100/500], Loss: 0.3629\n",
            "Epoch [23/200], Step [200/500], Loss: 0.4844\n",
            "Epoch [23/200], Step [300/500], Loss: 0.5102\n",
            "Epoch [23/200], Step [400/500], Loss: 0.6054\n",
            "Epoch [23/200], Step [500/500], Loss: 0.5190\n",
            "Epoch [24/200], Step [100/500], Loss: 0.4065\n",
            "Epoch [24/200], Step [200/500], Loss: 0.3716\n",
            "Epoch [24/200], Step [300/500], Loss: 0.2329\n",
            "Epoch [24/200], Step [400/500], Loss: 0.4663\n",
            "Epoch [24/200], Step [500/500], Loss: 0.4633\n",
            "Epoch [25/200], Step [100/500], Loss: 0.1893\n",
            "Epoch [25/200], Step [200/500], Loss: 0.7548\n",
            "Epoch [25/200], Step [300/500], Loss: 0.4585\n",
            "Epoch [25/200], Step [400/500], Loss: 0.5917\n",
            "Epoch [25/200], Step [500/500], Loss: 0.4265\n",
            "Epoch [26/200], Step [100/500], Loss: 0.4775\n",
            "Epoch [26/200], Step [200/500], Loss: 0.3162\n",
            "Epoch [26/200], Step [300/500], Loss: 0.6058\n",
            "Epoch [26/200], Step [400/500], Loss: 0.3511\n",
            "Epoch [26/200], Step [500/500], Loss: 0.5625\n",
            "Epoch [27/200], Step [100/500], Loss: 0.4575\n",
            "Epoch [27/200], Step [200/500], Loss: 0.5125\n",
            "Epoch [27/200], Step [300/500], Loss: 0.4476\n",
            "Epoch [27/200], Step [400/500], Loss: 0.3802\n",
            "Epoch [27/200], Step [500/500], Loss: 0.5007\n",
            "Epoch [28/200], Step [100/500], Loss: 0.2866\n",
            "Epoch [28/200], Step [200/500], Loss: 0.4704\n",
            "Epoch [28/200], Step [300/500], Loss: 0.4473\n",
            "Epoch [28/200], Step [400/500], Loss: 0.3320\n",
            "Epoch [28/200], Step [500/500], Loss: 0.4692\n",
            "Epoch [29/200], Step [100/500], Loss: 0.3088\n",
            "Epoch [29/200], Step [200/500], Loss: 0.4183\n",
            "Epoch [29/200], Step [300/500], Loss: 0.3959\n",
            "Epoch [29/200], Step [400/500], Loss: 0.3214\n",
            "Epoch [29/200], Step [500/500], Loss: 0.4889\n",
            "Epoch [30/200], Step [100/500], Loss: 0.4341\n",
            "Epoch [30/200], Step [200/500], Loss: 0.3367\n",
            "Epoch [30/200], Step [300/500], Loss: 0.4657\n",
            "Epoch [30/200], Step [400/500], Loss: 0.3841\n",
            "Epoch [30/200], Step [500/500], Loss: 0.4862\n",
            "Epoch [31/200], Step [100/500], Loss: 0.2799\n",
            "Epoch [31/200], Step [200/500], Loss: 0.4753\n",
            "Epoch [31/200], Step [300/500], Loss: 0.3633\n",
            "Epoch [31/200], Step [400/500], Loss: 0.6519\n",
            "Epoch [31/200], Step [500/500], Loss: 0.5159\n",
            "Epoch [32/200], Step [100/500], Loss: 0.5032\n",
            "Epoch [32/200], Step [200/500], Loss: 0.3733\n",
            "Epoch [32/200], Step [300/500], Loss: 0.4621\n",
            "Epoch [32/200], Step [400/500], Loss: 0.2026\n",
            "Epoch [32/200], Step [500/500], Loss: 0.4398\n",
            "Epoch [33/200], Step [100/500], Loss: 0.3004\n",
            "Epoch [33/200], Step [200/500], Loss: 0.4460\n",
            "Epoch [33/200], Step [300/500], Loss: 0.4104\n",
            "Epoch [33/200], Step [400/500], Loss: 0.2938\n",
            "Epoch [33/200], Step [500/500], Loss: 0.4611\n",
            "Epoch [34/200], Step [100/500], Loss: 0.3523\n",
            "Epoch [34/200], Step [200/500], Loss: 0.3386\n",
            "Epoch [34/200], Step [300/500], Loss: 0.5520\n",
            "Epoch [34/200], Step [400/500], Loss: 0.3117\n",
            "Epoch [34/200], Step [500/500], Loss: 0.3427\n",
            "Epoch [35/200], Step [100/500], Loss: 0.5280\n",
            "Epoch [35/200], Step [200/500], Loss: 0.6174\n",
            "Epoch [35/200], Step [300/500], Loss: 0.2684\n",
            "Epoch [35/200], Step [400/500], Loss: 0.4972\n",
            "Epoch [35/200], Step [500/500], Loss: 0.3547\n",
            "Epoch [36/200], Step [100/500], Loss: 0.2588\n",
            "Epoch [36/200], Step [200/500], Loss: 0.5678\n",
            "Epoch [36/200], Step [300/500], Loss: 0.5153\n",
            "Epoch [36/200], Step [400/500], Loss: 0.2988\n",
            "Epoch [36/200], Step [500/500], Loss: 0.5479\n",
            "Epoch [37/200], Step [100/500], Loss: 0.3153\n",
            "Epoch [37/200], Step [200/500], Loss: 0.6609\n",
            "Epoch [37/200], Step [300/500], Loss: 0.2807\n",
            "Epoch [37/200], Step [400/500], Loss: 0.3750\n",
            "Epoch [37/200], Step [500/500], Loss: 0.4186\n",
            "Epoch [38/200], Step [100/500], Loss: 0.4209\n",
            "Epoch [38/200], Step [200/500], Loss: 0.3283\n",
            "Epoch [38/200], Step [300/500], Loss: 0.5165\n",
            "Epoch [38/200], Step [400/500], Loss: 0.4366\n",
            "Epoch [38/200], Step [500/500], Loss: 0.2059\n",
            "Epoch [39/200], Step [100/500], Loss: 0.4184\n",
            "Epoch [39/200], Step [200/500], Loss: 0.4727\n",
            "Epoch [39/200], Step [300/500], Loss: 0.5447\n",
            "Epoch [39/200], Step [400/500], Loss: 0.4731\n",
            "Epoch [39/200], Step [500/500], Loss: 0.4928\n",
            "Epoch [40/200], Step [100/500], Loss: 0.4472\n",
            "Epoch [40/200], Step [200/500], Loss: 0.4717\n",
            "Epoch [40/200], Step [300/500], Loss: 0.3703\n",
            "Epoch [40/200], Step [400/500], Loss: 0.2954\n",
            "Epoch [40/200], Step [500/500], Loss: 0.3129\n",
            "Epoch [41/200], Step [100/500], Loss: 0.5222\n",
            "Epoch [41/200], Step [200/500], Loss: 0.4511\n",
            "Epoch [41/200], Step [300/500], Loss: 0.3634\n",
            "Epoch [41/200], Step [400/500], Loss: 0.4001\n",
            "Epoch [41/200], Step [500/500], Loss: 0.4837\n",
            "Epoch [42/200], Step [100/500], Loss: 0.1617\n",
            "Epoch [42/200], Step [200/500], Loss: 0.2242\n",
            "Epoch [42/200], Step [300/500], Loss: 0.4276\n",
            "Epoch [42/200], Step [400/500], Loss: 0.1882\n",
            "Epoch [42/200], Step [500/500], Loss: 0.2858\n",
            "Epoch [43/200], Step [100/500], Loss: 0.3354\n",
            "Epoch [43/200], Step [200/500], Loss: 0.3753\n",
            "Epoch [43/200], Step [300/500], Loss: 0.3706\n",
            "Epoch [43/200], Step [400/500], Loss: 0.4185\n",
            "Epoch [43/200], Step [500/500], Loss: 0.3488\n",
            "Epoch [44/200], Step [100/500], Loss: 0.6380\n",
            "Epoch [44/200], Step [200/500], Loss: 0.2174\n",
            "Epoch [44/200], Step [300/500], Loss: 0.1469\n",
            "Epoch [44/200], Step [400/500], Loss: 0.3095\n",
            "Epoch [44/200], Step [500/500], Loss: 0.2155\n",
            "Epoch [45/200], Step [100/500], Loss: 0.3769\n",
            "Epoch [45/200], Step [200/500], Loss: 0.2155\n",
            "Epoch [45/200], Step [300/500], Loss: 0.2731\n",
            "Epoch [45/200], Step [400/500], Loss: 0.4499\n",
            "Epoch [45/200], Step [500/500], Loss: 0.3839\n",
            "Epoch [46/200], Step [100/500], Loss: 0.5432\n",
            "Epoch [46/200], Step [200/500], Loss: 0.3013\n",
            "Epoch [46/200], Step [300/500], Loss: 0.5493\n",
            "Epoch [46/200], Step [400/500], Loss: 0.1565\n",
            "Epoch [46/200], Step [500/500], Loss: 0.2907\n",
            "Epoch [47/200], Step [100/500], Loss: 0.1734\n",
            "Epoch [47/200], Step [200/500], Loss: 0.4413\n",
            "Epoch [47/200], Step [300/500], Loss: 0.3344\n",
            "Epoch [47/200], Step [400/500], Loss: 0.4722\n",
            "Epoch [47/200], Step [500/500], Loss: 0.2652\n",
            "Epoch [48/200], Step [100/500], Loss: 0.4718\n",
            "Epoch [48/200], Step [200/500], Loss: 0.4827\n",
            "Epoch [48/200], Step [300/500], Loss: 0.3878\n",
            "Epoch [48/200], Step [400/500], Loss: 0.4108\n",
            "Epoch [48/200], Step [500/500], Loss: 0.3446\n",
            "Epoch [49/200], Step [100/500], Loss: 0.1882\n",
            "Epoch [49/200], Step [200/500], Loss: 0.3783\n",
            "Epoch [49/200], Step [300/500], Loss: 0.4077\n",
            "Epoch [49/200], Step [400/500], Loss: 0.3611\n",
            "Epoch [49/200], Step [500/500], Loss: 0.2941\n",
            "Epoch [50/200], Step [100/500], Loss: 0.2082\n",
            "Epoch [50/200], Step [200/500], Loss: 0.2614\n",
            "Epoch [50/200], Step [300/500], Loss: 0.2362\n",
            "Epoch [50/200], Step [400/500], Loss: 0.2988\n",
            "Epoch [50/200], Step [500/500], Loss: 0.2864\n",
            "Epoch [51/200], Step [100/500], Loss: 0.3781\n",
            "Epoch [51/200], Step [200/500], Loss: 0.3063\n",
            "Epoch [51/200], Step [300/500], Loss: 0.2244\n",
            "Epoch [51/200], Step [400/500], Loss: 0.3723\n",
            "Epoch [51/200], Step [500/500], Loss: 0.2318\n",
            "Epoch [52/200], Step [100/500], Loss: 0.3483\n",
            "Epoch [52/200], Step [200/500], Loss: 0.3364\n",
            "Epoch [52/200], Step [300/500], Loss: 0.3652\n",
            "Epoch [52/200], Step [400/500], Loss: 0.2458\n",
            "Epoch [52/200], Step [500/500], Loss: 0.1579\n",
            "Epoch [53/200], Step [100/500], Loss: 0.2958\n",
            "Epoch [53/200], Step [200/500], Loss: 0.2978\n",
            "Epoch [53/200], Step [300/500], Loss: 0.4976\n",
            "Epoch [53/200], Step [400/500], Loss: 0.3969\n",
            "Epoch [53/200], Step [500/500], Loss: 0.2944\n",
            "Epoch [54/200], Step [100/500], Loss: 0.4276\n",
            "Epoch [54/200], Step [200/500], Loss: 0.2499\n",
            "Epoch [54/200], Step [300/500], Loss: 0.4073\n",
            "Epoch [54/200], Step [400/500], Loss: 0.4150\n",
            "Epoch [54/200], Step [500/500], Loss: 0.4545\n",
            "Epoch [55/200], Step [100/500], Loss: 0.1954\n",
            "Epoch [55/200], Step [200/500], Loss: 0.2241\n",
            "Epoch [55/200], Step [300/500], Loss: 0.1856\n",
            "Epoch [55/200], Step [400/500], Loss: 0.3396\n",
            "Epoch [55/200], Step [500/500], Loss: 0.2996\n",
            "Epoch [56/200], Step [100/500], Loss: 0.4464\n",
            "Epoch [56/200], Step [200/500], Loss: 0.3451\n",
            "Epoch [56/200], Step [300/500], Loss: 0.3150\n",
            "Epoch [56/200], Step [400/500], Loss: 0.3725\n",
            "Epoch [56/200], Step [500/500], Loss: 0.2491\n",
            "Epoch [57/200], Step [100/500], Loss: 0.3004\n",
            "Epoch [57/200], Step [200/500], Loss: 0.3913\n",
            "Epoch [57/200], Step [300/500], Loss: 0.2180\n",
            "Epoch [57/200], Step [400/500], Loss: 0.2419\n",
            "Epoch [57/200], Step [500/500], Loss: 0.2954\n",
            "Epoch [58/200], Step [100/500], Loss: 0.2095\n",
            "Epoch [58/200], Step [200/500], Loss: 0.4265\n",
            "Epoch [58/200], Step [300/500], Loss: 0.3267\n",
            "Epoch [58/200], Step [400/500], Loss: 0.3473\n",
            "Epoch [58/200], Step [500/500], Loss: 0.3142\n",
            "Epoch [59/200], Step [100/500], Loss: 0.3737\n",
            "Epoch [59/200], Step [200/500], Loss: 0.3147\n",
            "Epoch [59/200], Step [300/500], Loss: 0.3877\n",
            "Epoch [59/200], Step [400/500], Loss: 0.3416\n",
            "Epoch [59/200], Step [500/500], Loss: 0.2913\n",
            "Epoch [60/200], Step [100/500], Loss: 0.2770\n",
            "Epoch [60/200], Step [200/500], Loss: 0.3107\n",
            "Epoch [60/200], Step [300/500], Loss: 0.2923\n",
            "Epoch [60/200], Step [400/500], Loss: 0.2810\n",
            "Epoch [60/200], Step [500/500], Loss: 0.4864\n",
            "Epoch [61/200], Step [100/500], Loss: 0.2912\n",
            "Epoch [61/200], Step [200/500], Loss: 0.2031\n",
            "Epoch [61/200], Step [300/500], Loss: 0.2276\n",
            "Epoch [61/200], Step [400/500], Loss: 0.2487\n",
            "Epoch [61/200], Step [500/500], Loss: 0.4317\n",
            "Epoch [62/200], Step [100/500], Loss: 0.3447\n",
            "Epoch [62/200], Step [200/500], Loss: 0.2805\n",
            "Epoch [62/200], Step [300/500], Loss: 0.5683\n",
            "Epoch [62/200], Step [400/500], Loss: 0.1692\n",
            "Epoch [62/200], Step [500/500], Loss: 0.4038\n",
            "Epoch [63/200], Step [100/500], Loss: 0.2050\n",
            "Epoch [63/200], Step [200/500], Loss: 0.4934\n",
            "Epoch [63/200], Step [300/500], Loss: 0.2893\n",
            "Epoch [63/200], Step [400/500], Loss: 0.3147\n",
            "Epoch [63/200], Step [500/500], Loss: 0.4659\n",
            "Epoch [64/200], Step [100/500], Loss: 0.3467\n",
            "Epoch [64/200], Step [200/500], Loss: 0.2283\n",
            "Epoch [64/200], Step [300/500], Loss: 0.2596\n",
            "Epoch [64/200], Step [400/500], Loss: 0.3288\n",
            "Epoch [64/200], Step [500/500], Loss: 0.5403\n",
            "Epoch [65/200], Step [100/500], Loss: 0.4878\n",
            "Epoch [65/200], Step [200/500], Loss: 0.3242\n",
            "Epoch [65/200], Step [300/500], Loss: 0.3550\n",
            "Epoch [65/200], Step [400/500], Loss: 0.3927\n",
            "Epoch [65/200], Step [500/500], Loss: 0.2820\n",
            "Epoch [66/200], Step [100/500], Loss: 0.2702\n",
            "Epoch [66/200], Step [200/500], Loss: 0.2752\n",
            "Epoch [66/200], Step [300/500], Loss: 0.4378\n",
            "Epoch [66/200], Step [400/500], Loss: 0.3773\n",
            "Epoch [66/200], Step [500/500], Loss: 0.3586\n",
            "Epoch [67/200], Step [100/500], Loss: 0.2490\n",
            "Epoch [67/200], Step [200/500], Loss: 0.3880\n",
            "Epoch [67/200], Step [300/500], Loss: 0.1690\n",
            "Epoch [67/200], Step [400/500], Loss: 0.3806\n",
            "Epoch [67/200], Step [500/500], Loss: 0.2719\n",
            "Epoch [68/200], Step [100/500], Loss: 0.2854\n",
            "Epoch [68/200], Step [200/500], Loss: 0.6003\n",
            "Epoch [68/200], Step [300/500], Loss: 0.2508\n",
            "Epoch [68/200], Step [400/500], Loss: 0.2859\n",
            "Epoch [68/200], Step [500/500], Loss: 0.2930\n",
            "Epoch [69/200], Step [100/500], Loss: 0.2626\n",
            "Epoch [69/200], Step [200/500], Loss: 0.2203\n",
            "Epoch [69/200], Step [300/500], Loss: 0.2634\n",
            "Epoch [69/200], Step [400/500], Loss: 0.2407\n",
            "Epoch [69/200], Step [500/500], Loss: 0.3528\n",
            "Epoch [70/200], Step [100/500], Loss: 0.1825\n",
            "Epoch [70/200], Step [200/500], Loss: 0.2722\n",
            "Epoch [70/200], Step [300/500], Loss: 0.2973\n",
            "Epoch [70/200], Step [400/500], Loss: 0.2881\n",
            "Epoch [70/200], Step [500/500], Loss: 0.3095\n",
            "Epoch [71/200], Step [100/500], Loss: 0.3080\n",
            "Epoch [71/200], Step [200/500], Loss: 0.2861\n",
            "Epoch [71/200], Step [300/500], Loss: 0.2942\n",
            "Epoch [71/200], Step [400/500], Loss: 0.2286\n",
            "Epoch [71/200], Step [500/500], Loss: 0.2116\n",
            "Epoch [72/200], Step [100/500], Loss: 0.3771\n",
            "Epoch [72/200], Step [200/500], Loss: 0.3106\n",
            "Epoch [72/200], Step [300/500], Loss: 0.2651\n",
            "Epoch [72/200], Step [400/500], Loss: 0.5330\n",
            "Epoch [72/200], Step [500/500], Loss: 0.3098\n",
            "Epoch [73/200], Step [100/500], Loss: 0.2907\n",
            "Epoch [73/200], Step [200/500], Loss: 0.3956\n",
            "Epoch [73/200], Step [300/500], Loss: 0.2144\n",
            "Epoch [73/200], Step [400/500], Loss: 0.2984\n",
            "Epoch [73/200], Step [500/500], Loss: 0.0965\n",
            "Epoch [74/200], Step [100/500], Loss: 0.1593\n",
            "Epoch [74/200], Step [200/500], Loss: 0.2885\n",
            "Epoch [74/200], Step [300/500], Loss: 0.2679\n",
            "Epoch [74/200], Step [400/500], Loss: 0.3252\n",
            "Epoch [74/200], Step [500/500], Loss: 0.2393\n",
            "Epoch [75/200], Step [100/500], Loss: 0.2224\n",
            "Epoch [75/200], Step [200/500], Loss: 0.3506\n",
            "Epoch [75/200], Step [300/500], Loss: 0.1952\n",
            "Epoch [75/200], Step [400/500], Loss: 0.3236\n",
            "Epoch [75/200], Step [500/500], Loss: 0.3635\n",
            "Epoch [76/200], Step [100/500], Loss: 0.1718\n",
            "Epoch [76/200], Step [200/500], Loss: 0.4094\n",
            "Epoch [76/200], Step [300/500], Loss: 0.3513\n",
            "Epoch [76/200], Step [400/500], Loss: 0.2427\n",
            "Epoch [76/200], Step [500/500], Loss: 0.3407\n",
            "Epoch [77/200], Step [100/500], Loss: 0.2647\n",
            "Epoch [77/200], Step [200/500], Loss: 0.2773\n",
            "Epoch [77/200], Step [300/500], Loss: 0.3569\n",
            "Epoch [77/200], Step [400/500], Loss: 0.2750\n",
            "Epoch [77/200], Step [500/500], Loss: 0.3337\n",
            "Epoch [78/200], Step [100/500], Loss: 0.2003\n",
            "Epoch [78/200], Step [200/500], Loss: 0.3477\n",
            "Epoch [78/200], Step [300/500], Loss: 0.2753\n",
            "Epoch [78/200], Step [400/500], Loss: 0.3544\n",
            "Epoch [78/200], Step [500/500], Loss: 0.2919\n",
            "Epoch [79/200], Step [100/500], Loss: 0.2036\n",
            "Epoch [79/200], Step [200/500], Loss: 0.2570\n",
            "Epoch [79/200], Step [300/500], Loss: 0.2903\n",
            "Epoch [79/200], Step [400/500], Loss: 0.1487\n",
            "Epoch [79/200], Step [500/500], Loss: 0.2055\n",
            "Epoch [80/200], Step [100/500], Loss: 0.2962\n",
            "Epoch [80/200], Step [200/500], Loss: 0.2071\n",
            "Epoch [80/200], Step [300/500], Loss: 0.2410\n",
            "Epoch [80/200], Step [400/500], Loss: 0.2692\n",
            "Epoch [80/200], Step [500/500], Loss: 0.3363\n",
            "Epoch [81/200], Step [100/500], Loss: 0.1109\n",
            "Epoch [81/200], Step [200/500], Loss: 0.3809\n",
            "Epoch [81/200], Step [300/500], Loss: 0.5102\n",
            "Epoch [81/200], Step [400/500], Loss: 0.3187\n",
            "Epoch [81/200], Step [500/500], Loss: 0.1957\n",
            "Epoch [82/200], Step [100/500], Loss: 0.3394\n",
            "Epoch [82/200], Step [200/500], Loss: 0.2947\n",
            "Epoch [82/200], Step [300/500], Loss: 0.1292\n",
            "Epoch [82/200], Step [400/500], Loss: 0.3151\n",
            "Epoch [82/200], Step [500/500], Loss: 0.5735\n",
            "Epoch [83/200], Step [100/500], Loss: 0.3051\n",
            "Epoch [83/200], Step [200/500], Loss: 0.3180\n",
            "Epoch [83/200], Step [300/500], Loss: 0.4032\n",
            "Epoch [83/200], Step [400/500], Loss: 0.4256\n",
            "Epoch [83/200], Step [500/500], Loss: 0.2418\n",
            "Epoch [84/200], Step [100/500], Loss: 0.1051\n",
            "Epoch [84/200], Step [200/500], Loss: 0.0858\n",
            "Epoch [84/200], Step [300/500], Loss: 0.2359\n",
            "Epoch [84/200], Step [400/500], Loss: 0.1955\n",
            "Epoch [84/200], Step [500/500], Loss: 0.1775\n",
            "Epoch [85/200], Step [100/500], Loss: 0.3577\n",
            "Epoch [85/200], Step [200/500], Loss: 0.3778\n",
            "Epoch [85/200], Step [300/500], Loss: 0.2442\n",
            "Epoch [85/200], Step [400/500], Loss: 0.3213\n",
            "Epoch [85/200], Step [500/500], Loss: 0.3822\n",
            "Epoch [86/200], Step [100/500], Loss: 0.4033\n",
            "Epoch [86/200], Step [200/500], Loss: 0.4197\n",
            "Epoch [86/200], Step [300/500], Loss: 0.2521\n",
            "Epoch [86/200], Step [400/500], Loss: 0.3634\n",
            "Epoch [86/200], Step [500/500], Loss: 0.2827\n",
            "Epoch [87/200], Step [100/500], Loss: 0.1512\n",
            "Epoch [87/200], Step [200/500], Loss: 0.2926\n",
            "Epoch [87/200], Step [300/500], Loss: 0.3765\n",
            "Epoch [87/200], Step [400/500], Loss: 0.1343\n",
            "Epoch [87/200], Step [500/500], Loss: 0.2311\n",
            "Epoch [88/200], Step [100/500], Loss: 0.2055\n",
            "Epoch [88/200], Step [200/500], Loss: 0.4646\n",
            "Epoch [88/200], Step [300/500], Loss: 0.2785\n",
            "Epoch [88/200], Step [400/500], Loss: 0.2737\n",
            "Epoch [88/200], Step [500/500], Loss: 0.3144\n",
            "Epoch [89/200], Step [100/500], Loss: 0.2831\n",
            "Epoch [89/200], Step [200/500], Loss: 0.3962\n",
            "Epoch [89/200], Step [300/500], Loss: 0.2043\n",
            "Epoch [89/200], Step [400/500], Loss: 0.5084\n",
            "Epoch [89/200], Step [500/500], Loss: 0.3203\n",
            "Epoch [90/200], Step [100/500], Loss: 0.2575\n",
            "Epoch [90/200], Step [200/500], Loss: 0.3142\n",
            "Epoch [90/200], Step [300/500], Loss: 0.3360\n",
            "Epoch [90/200], Step [400/500], Loss: 0.3908\n",
            "Epoch [90/200], Step [500/500], Loss: 0.1445\n",
            "Epoch [91/200], Step [100/500], Loss: 0.2975\n",
            "Epoch [91/200], Step [200/500], Loss: 0.2110\n",
            "Epoch [91/200], Step [300/500], Loss: 0.1367\n",
            "Epoch [91/200], Step [400/500], Loss: 0.3089\n",
            "Epoch [91/200], Step [500/500], Loss: 0.3550\n",
            "Epoch [92/200], Step [100/500], Loss: 0.2368\n",
            "Epoch [92/200], Step [200/500], Loss: 0.5483\n",
            "Epoch [92/200], Step [300/500], Loss: 0.3140\n",
            "Epoch [92/200], Step [400/500], Loss: 0.3235\n",
            "Epoch [92/200], Step [500/500], Loss: 0.2768\n",
            "Epoch [93/200], Step [100/500], Loss: 0.1618\n",
            "Epoch [93/200], Step [200/500], Loss: 0.3291\n",
            "Epoch [93/200], Step [300/500], Loss: 0.3176\n",
            "Epoch [93/200], Step [400/500], Loss: 0.2516\n",
            "Epoch [93/200], Step [500/500], Loss: 0.1347\n",
            "Epoch [94/200], Step [100/500], Loss: 0.3320\n",
            "Epoch [94/200], Step [200/500], Loss: 0.4234\n",
            "Epoch [94/200], Step [300/500], Loss: 0.2214\n",
            "Epoch [94/200], Step [400/500], Loss: 0.2910\n",
            "Epoch [94/200], Step [500/500], Loss: 0.2088\n",
            "Epoch [95/200], Step [100/500], Loss: 0.1370\n",
            "Epoch [95/200], Step [200/500], Loss: 0.3027\n",
            "Epoch [95/200], Step [300/500], Loss: 0.2241\n",
            "Epoch [95/200], Step [400/500], Loss: 0.2651\n",
            "Epoch [95/200], Step [500/500], Loss: 0.1775\n",
            "Epoch [96/200], Step [100/500], Loss: 0.2252\n",
            "Epoch [96/200], Step [200/500], Loss: 0.2211\n",
            "Epoch [96/200], Step [300/500], Loss: 0.2883\n",
            "Epoch [96/200], Step [400/500], Loss: 0.1859\n",
            "Epoch [96/200], Step [500/500], Loss: 0.4319\n",
            "Epoch [97/200], Step [100/500], Loss: 0.2009\n",
            "Epoch [97/200], Step [200/500], Loss: 0.3002\n",
            "Epoch [97/200], Step [300/500], Loss: 0.3953\n",
            "Epoch [97/200], Step [400/500], Loss: 0.3017\n",
            "Epoch [97/200], Step [500/500], Loss: 0.3424\n",
            "Epoch [98/200], Step [100/500], Loss: 0.2629\n",
            "Epoch [98/200], Step [200/500], Loss: 0.2560\n",
            "Epoch [98/200], Step [300/500], Loss: 0.2704\n",
            "Epoch [98/200], Step [400/500], Loss: 0.2723\n",
            "Epoch [98/200], Step [500/500], Loss: 0.2180\n",
            "Epoch [99/200], Step [100/500], Loss: 0.3132\n",
            "Epoch [99/200], Step [200/500], Loss: 0.1469\n",
            "Epoch [99/200], Step [300/500], Loss: 0.4172\n",
            "Epoch [99/200], Step [400/500], Loss: 0.3983\n",
            "Epoch [99/200], Step [500/500], Loss: 0.2913\n",
            "Epoch [100/200], Step [100/500], Loss: 0.2571\n",
            "Epoch [100/200], Step [200/500], Loss: 0.2716\n",
            "Epoch [100/200], Step [300/500], Loss: 0.4265\n",
            "Epoch [100/200], Step [400/500], Loss: 0.1591\n",
            "Epoch [100/200], Step [500/500], Loss: 0.4113\n",
            "Epoch [101/200], Step [100/500], Loss: 0.2259\n",
            "Epoch [101/200], Step [200/500], Loss: 0.3218\n",
            "Epoch [101/200], Step [300/500], Loss: 0.3767\n",
            "Epoch [101/200], Step [400/500], Loss: 0.2211\n",
            "Epoch [101/200], Step [500/500], Loss: 0.3090\n",
            "Epoch [102/200], Step [100/500], Loss: 0.2751\n",
            "Epoch [102/200], Step [200/500], Loss: 0.3263\n",
            "Epoch [102/200], Step [300/500], Loss: 0.2382\n",
            "Epoch [102/200], Step [400/500], Loss: 0.3241\n",
            "Epoch [102/200], Step [500/500], Loss: 0.3639\n",
            "Epoch [103/200], Step [100/500], Loss: 0.4862\n",
            "Epoch [103/200], Step [200/500], Loss: 0.4340\n",
            "Epoch [103/200], Step [300/500], Loss: 0.1732\n",
            "Epoch [103/200], Step [400/500], Loss: 0.3034\n",
            "Epoch [103/200], Step [500/500], Loss: 0.3212\n",
            "Epoch [104/200], Step [100/500], Loss: 0.1948\n",
            "Epoch [104/200], Step [200/500], Loss: 0.1227\n",
            "Epoch [104/200], Step [300/500], Loss: 0.2283\n",
            "Epoch [104/200], Step [400/500], Loss: 0.3307\n",
            "Epoch [104/200], Step [500/500], Loss: 0.4011\n",
            "Epoch [105/200], Step [100/500], Loss: 0.2767\n",
            "Epoch [105/200], Step [200/500], Loss: 0.3010\n",
            "Epoch [105/200], Step [300/500], Loss: 0.2549\n",
            "Epoch [105/200], Step [400/500], Loss: 0.2699\n",
            "Epoch [105/200], Step [500/500], Loss: 0.3361\n",
            "Epoch [106/200], Step [100/500], Loss: 0.1489\n",
            "Epoch [106/200], Step [200/500], Loss: 0.1922\n",
            "Epoch [106/200], Step [300/500], Loss: 0.2521\n",
            "Epoch [106/200], Step [400/500], Loss: 0.3227\n",
            "Epoch [106/200], Step [500/500], Loss: 0.2263\n",
            "Epoch [107/200], Step [100/500], Loss: 0.2451\n",
            "Epoch [107/200], Step [200/500], Loss: 0.1942\n",
            "Epoch [107/200], Step [300/500], Loss: 0.3126\n",
            "Epoch [107/200], Step [400/500], Loss: 0.3206\n",
            "Epoch [107/200], Step [500/500], Loss: 0.2773\n",
            "Epoch [108/200], Step [100/500], Loss: 0.1902\n",
            "Epoch [108/200], Step [200/500], Loss: 0.3785\n",
            "Epoch [108/200], Step [300/500], Loss: 0.1857\n",
            "Epoch [108/200], Step [400/500], Loss: 0.3296\n",
            "Epoch [108/200], Step [500/500], Loss: 0.3298\n",
            "Epoch [109/200], Step [100/500], Loss: 0.2344\n",
            "Epoch [109/200], Step [200/500], Loss: 0.3972\n",
            "Epoch [109/200], Step [300/500], Loss: 0.4021\n",
            "Epoch [109/200], Step [400/500], Loss: 0.2711\n",
            "Epoch [109/200], Step [500/500], Loss: 0.2785\n",
            "Epoch [110/200], Step [100/500], Loss: 0.2171\n",
            "Epoch [110/200], Step [200/500], Loss: 0.3271\n",
            "Epoch [110/200], Step [300/500], Loss: 0.2808\n",
            "Epoch [110/200], Step [400/500], Loss: 0.2062\n",
            "Epoch [110/200], Step [500/500], Loss: 0.1359\n",
            "Epoch [111/200], Step [100/500], Loss: 0.2246\n",
            "Epoch [111/200], Step [200/500], Loss: 0.2734\n",
            "Epoch [111/200], Step [300/500], Loss: 0.3394\n",
            "Epoch [111/200], Step [400/500], Loss: 0.2246\n",
            "Epoch [111/200], Step [500/500], Loss: 0.2577\n",
            "Epoch [112/200], Step [100/500], Loss: 0.4042\n",
            "Epoch [112/200], Step [200/500], Loss: 0.3481\n",
            "Epoch [112/200], Step [300/500], Loss: 0.1992\n",
            "Epoch [112/200], Step [400/500], Loss: 0.1797\n",
            "Epoch [112/200], Step [500/500], Loss: 0.3055\n",
            "Epoch [113/200], Step [100/500], Loss: 0.1690\n",
            "Epoch [113/200], Step [200/500], Loss: 0.2976\n",
            "Epoch [113/200], Step [300/500], Loss: 0.1512\n",
            "Epoch [113/200], Step [400/500], Loss: 0.2001\n",
            "Epoch [113/200], Step [500/500], Loss: 0.4567\n",
            "Epoch [114/200], Step [100/500], Loss: 0.2702\n",
            "Epoch [114/200], Step [200/500], Loss: 0.1579\n",
            "Epoch [114/200], Step [300/500], Loss: 0.2107\n",
            "Epoch [114/200], Step [400/500], Loss: 0.2729\n",
            "Epoch [114/200], Step [500/500], Loss: 0.2116\n",
            "Epoch [115/200], Step [100/500], Loss: 0.3606\n",
            "Epoch [115/200], Step [200/500], Loss: 0.2116\n",
            "Epoch [115/200], Step [300/500], Loss: 0.2666\n",
            "Epoch [115/200], Step [400/500], Loss: 0.3021\n",
            "Epoch [115/200], Step [500/500], Loss: 0.2646\n",
            "Epoch [116/200], Step [100/500], Loss: 0.3301\n",
            "Epoch [116/200], Step [200/500], Loss: 0.1673\n",
            "Epoch [116/200], Step [300/500], Loss: 0.2246\n",
            "Epoch [116/200], Step [400/500], Loss: 0.4189\n",
            "Epoch [116/200], Step [500/500], Loss: 0.2637\n",
            "Epoch [117/200], Step [100/500], Loss: 0.2516\n",
            "Epoch [117/200], Step [200/500], Loss: 0.1384\n",
            "Epoch [117/200], Step [300/500], Loss: 0.1642\n",
            "Epoch [117/200], Step [400/500], Loss: 0.2094\n",
            "Epoch [117/200], Step [500/500], Loss: 0.1958\n",
            "Epoch [118/200], Step [100/500], Loss: 0.2575\n",
            "Epoch [118/200], Step [200/500], Loss: 0.3119\n",
            "Epoch [118/200], Step [300/500], Loss: 0.2337\n",
            "Epoch [118/200], Step [400/500], Loss: 0.3400\n",
            "Epoch [118/200], Step [500/500], Loss: 0.1868\n",
            "Epoch [119/200], Step [100/500], Loss: 0.2153\n",
            "Epoch [119/200], Step [200/500], Loss: 0.3182\n",
            "Epoch [119/200], Step [300/500], Loss: 0.3205\n",
            "Epoch [119/200], Step [400/500], Loss: 0.2623\n",
            "Epoch [119/200], Step [500/500], Loss: 0.2893\n",
            "Epoch [120/200], Step [100/500], Loss: 0.1619\n",
            "Epoch [120/200], Step [200/500], Loss: 0.3045\n",
            "Epoch [120/200], Step [300/500], Loss: 0.2742\n",
            "Epoch [120/200], Step [400/500], Loss: 0.1990\n",
            "Epoch [120/200], Step [500/500], Loss: 0.2332\n",
            "Epoch [121/200], Step [100/500], Loss: 0.2065\n",
            "Epoch [121/200], Step [200/500], Loss: 0.1814\n",
            "Epoch [121/200], Step [300/500], Loss: 0.1852\n",
            "Epoch [121/200], Step [400/500], Loss: 0.1856\n",
            "Epoch [121/200], Step [500/500], Loss: 0.1289\n",
            "Epoch [122/200], Step [100/500], Loss: 0.1927\n",
            "Epoch [122/200], Step [200/500], Loss: 0.2403\n",
            "Epoch [122/200], Step [300/500], Loss: 0.2777\n",
            "Epoch [122/200], Step [400/500], Loss: 0.1293\n",
            "Epoch [122/200], Step [500/500], Loss: 0.1665\n",
            "Epoch [123/200], Step [100/500], Loss: 0.1807\n",
            "Epoch [123/200], Step [200/500], Loss: 0.3589\n",
            "Epoch [123/200], Step [300/500], Loss: 0.1334\n",
            "Epoch [123/200], Step [400/500], Loss: 0.2640\n",
            "Epoch [123/200], Step [500/500], Loss: 0.1678\n",
            "Epoch [124/200], Step [100/500], Loss: 0.2741\n",
            "Epoch [124/200], Step [200/500], Loss: 0.2643\n",
            "Epoch [124/200], Step [300/500], Loss: 0.1627\n",
            "Epoch [124/200], Step [400/500], Loss: 0.3388\n",
            "Epoch [124/200], Step [500/500], Loss: 0.3246\n",
            "Epoch [125/200], Step [100/500], Loss: 0.1154\n",
            "Epoch [125/200], Step [200/500], Loss: 0.2095\n",
            "Epoch [125/200], Step [300/500], Loss: 0.2447\n",
            "Epoch [125/200], Step [400/500], Loss: 0.3622\n",
            "Epoch [125/200], Step [500/500], Loss: 0.1983\n",
            "Epoch [126/200], Step [100/500], Loss: 0.1256\n",
            "Epoch [126/200], Step [200/500], Loss: 0.4398\n",
            "Epoch [126/200], Step [300/500], Loss: 0.1263\n",
            "Epoch [126/200], Step [400/500], Loss: 0.1637\n",
            "Epoch [126/200], Step [500/500], Loss: 0.1302\n",
            "Epoch [127/200], Step [100/500], Loss: 0.1432\n",
            "Epoch [127/200], Step [200/500], Loss: 0.1583\n",
            "Epoch [127/200], Step [300/500], Loss: 0.2528\n",
            "Epoch [127/200], Step [400/500], Loss: 0.3603\n",
            "Epoch [127/200], Step [500/500], Loss: 0.1509\n",
            "Epoch [128/200], Step [100/500], Loss: 0.2977\n",
            "Epoch [128/200], Step [200/500], Loss: 0.1927\n",
            "Epoch [128/200], Step [300/500], Loss: 0.2338\n",
            "Epoch [128/200], Step [400/500], Loss: 0.2514\n",
            "Epoch [128/200], Step [500/500], Loss: 0.1614\n",
            "Epoch [129/200], Step [100/500], Loss: 0.2321\n",
            "Epoch [129/200], Step [200/500], Loss: 0.2895\n",
            "Epoch [129/200], Step [300/500], Loss: 0.2042\n",
            "Epoch [129/200], Step [400/500], Loss: 0.2355\n",
            "Epoch [129/200], Step [500/500], Loss: 0.2444\n",
            "Epoch [130/200], Step [100/500], Loss: 0.1176\n",
            "Epoch [130/200], Step [200/500], Loss: 0.3291\n",
            "Epoch [130/200], Step [300/500], Loss: 0.5075\n",
            "Epoch [130/200], Step [400/500], Loss: 0.4395\n",
            "Epoch [130/200], Step [500/500], Loss: 0.2088\n",
            "Epoch [131/200], Step [100/500], Loss: 0.2835\n",
            "Epoch [131/200], Step [200/500], Loss: 0.1966\n",
            "Epoch [131/200], Step [300/500], Loss: 0.1969\n",
            "Epoch [131/200], Step [400/500], Loss: 0.4642\n",
            "Epoch [131/200], Step [500/500], Loss: 0.1393\n",
            "Epoch [132/200], Step [100/500], Loss: 0.1517\n",
            "Epoch [132/200], Step [200/500], Loss: 0.3060\n",
            "Epoch [132/200], Step [300/500], Loss: 0.1803\n",
            "Epoch [132/200], Step [400/500], Loss: 0.1954\n",
            "Epoch [132/200], Step [500/500], Loss: 0.1211\n",
            "Epoch [133/200], Step [100/500], Loss: 0.1362\n",
            "Epoch [133/200], Step [200/500], Loss: 0.2154\n",
            "Epoch [133/200], Step [300/500], Loss: 0.2223\n",
            "Epoch [133/200], Step [400/500], Loss: 0.2601\n",
            "Epoch [133/200], Step [500/500], Loss: 0.2709\n",
            "Epoch [134/200], Step [100/500], Loss: 0.1891\n",
            "Epoch [134/200], Step [200/500], Loss: 0.4038\n",
            "Epoch [134/200], Step [300/500], Loss: 0.3342\n",
            "Epoch [134/200], Step [400/500], Loss: 0.3826\n",
            "Epoch [134/200], Step [500/500], Loss: 0.6997\n",
            "Epoch [135/200], Step [100/500], Loss: 0.1871\n",
            "Epoch [135/200], Step [200/500], Loss: 0.2592\n",
            "Epoch [135/200], Step [300/500], Loss: 0.0615\n",
            "Epoch [135/200], Step [400/500], Loss: 0.1252\n",
            "Epoch [135/200], Step [500/500], Loss: 0.2713\n",
            "Epoch [136/200], Step [100/500], Loss: 0.1798\n",
            "Epoch [136/200], Step [200/500], Loss: 0.2131\n",
            "Epoch [136/200], Step [300/500], Loss: 0.2531\n",
            "Epoch [136/200], Step [400/500], Loss: 0.1325\n",
            "Epoch [136/200], Step [500/500], Loss: 0.1812\n",
            "Epoch [137/200], Step [100/500], Loss: 0.1281\n",
            "Epoch [137/200], Step [200/500], Loss: 0.2011\n",
            "Epoch [137/200], Step [300/500], Loss: 0.2004\n",
            "Epoch [137/200], Step [400/500], Loss: 0.2784\n",
            "Epoch [137/200], Step [500/500], Loss: 0.2568\n",
            "Epoch [138/200], Step [100/500], Loss: 0.2704\n",
            "Epoch [138/200], Step [200/500], Loss: 0.3070\n",
            "Epoch [138/200], Step [300/500], Loss: 0.1801\n",
            "Epoch [138/200], Step [400/500], Loss: 0.1523\n",
            "Epoch [138/200], Step [500/500], Loss: 0.2532\n",
            "Epoch [139/200], Step [100/500], Loss: 0.1840\n",
            "Epoch [139/200], Step [200/500], Loss: 0.1484\n",
            "Epoch [139/200], Step [300/500], Loss: 0.2540\n",
            "Epoch [139/200], Step [400/500], Loss: 0.2032\n",
            "Epoch [139/200], Step [500/500], Loss: 0.2061\n",
            "Epoch [140/200], Step [100/500], Loss: 0.0808\n",
            "Epoch [140/200], Step [200/500], Loss: 0.1232\n",
            "Epoch [140/200], Step [300/500], Loss: 0.2694\n",
            "Epoch [140/200], Step [400/500], Loss: 0.2515\n",
            "Epoch [140/200], Step [500/500], Loss: 0.1072\n",
            "Epoch [141/200], Step [100/500], Loss: 0.2273\n",
            "Epoch [141/200], Step [200/500], Loss: 0.2432\n",
            "Epoch [141/200], Step [300/500], Loss: 0.2253\n",
            "Epoch [141/200], Step [400/500], Loss: 0.2834\n",
            "Epoch [141/200], Step [500/500], Loss: 0.1897\n",
            "Epoch [142/200], Step [100/500], Loss: 0.1786\n",
            "Epoch [142/200], Step [200/500], Loss: 0.2535\n",
            "Epoch [142/200], Step [300/500], Loss: 0.3252\n",
            "Epoch [142/200], Step [400/500], Loss: 0.3758\n",
            "Epoch [142/200], Step [500/500], Loss: 0.2478\n",
            "Epoch [143/200], Step [100/500], Loss: 0.0917\n",
            "Epoch [143/200], Step [200/500], Loss: 0.2905\n",
            "Epoch [143/200], Step [300/500], Loss: 0.2069\n",
            "Epoch [143/200], Step [400/500], Loss: 0.1462\n",
            "Epoch [143/200], Step [500/500], Loss: 0.2693\n",
            "Epoch [144/200], Step [100/500], Loss: 0.1311\n",
            "Epoch [144/200], Step [200/500], Loss: 0.4121\n",
            "Epoch [144/200], Step [300/500], Loss: 0.3296\n",
            "Epoch [144/200], Step [400/500], Loss: 0.1621\n",
            "Epoch [144/200], Step [500/500], Loss: 0.2229\n",
            "Epoch [145/200], Step [100/500], Loss: 0.1968\n",
            "Epoch [145/200], Step [200/500], Loss: 0.2417\n",
            "Epoch [145/200], Step [300/500], Loss: 0.1369\n",
            "Epoch [145/200], Step [400/500], Loss: 0.2263\n",
            "Epoch [145/200], Step [500/500], Loss: 0.3091\n",
            "Epoch [146/200], Step [100/500], Loss: 0.1790\n",
            "Epoch [146/200], Step [200/500], Loss: 0.3601\n",
            "Epoch [146/200], Step [300/500], Loss: 0.1383\n",
            "Epoch [146/200], Step [400/500], Loss: 0.0960\n",
            "Epoch [146/200], Step [500/500], Loss: 0.2009\n",
            "Epoch [147/200], Step [100/500], Loss: 0.3927\n",
            "Epoch [147/200], Step [200/500], Loss: 0.1306\n",
            "Epoch [147/200], Step [300/500], Loss: 0.1687\n",
            "Epoch [147/200], Step [400/500], Loss: 0.2212\n",
            "Epoch [147/200], Step [500/500], Loss: 0.3018\n",
            "Epoch [148/200], Step [100/500], Loss: 0.2176\n",
            "Epoch [148/200], Step [200/500], Loss: 0.1687\n",
            "Epoch [148/200], Step [300/500], Loss: 0.2810\n",
            "Epoch [148/200], Step [400/500], Loss: 0.6048\n",
            "Epoch [148/200], Step [500/500], Loss: 0.2138\n",
            "Epoch [149/200], Step [100/500], Loss: 0.1831\n",
            "Epoch [149/200], Step [200/500], Loss: 0.1256\n",
            "Epoch [149/200], Step [300/500], Loss: 0.2016\n",
            "Epoch [149/200], Step [400/500], Loss: 0.3707\n",
            "Epoch [149/200], Step [500/500], Loss: 0.1529\n",
            "Epoch [150/200], Step [100/500], Loss: 0.2151\n",
            "Epoch [150/200], Step [200/500], Loss: 0.2853\n",
            "Epoch [150/200], Step [300/500], Loss: 0.2224\n",
            "Epoch [150/200], Step [400/500], Loss: 0.2098\n",
            "Epoch [150/200], Step [500/500], Loss: 0.1783\n",
            "Epoch [151/200], Step [100/500], Loss: 0.2555\n",
            "Epoch [151/200], Step [200/500], Loss: 0.0408\n",
            "Epoch [151/200], Step [300/500], Loss: 0.1595\n",
            "Epoch [151/200], Step [400/500], Loss: 0.2393\n",
            "Epoch [151/200], Step [500/500], Loss: 0.2704\n",
            "Epoch [152/200], Step [100/500], Loss: 0.1829\n",
            "Epoch [152/200], Step [200/500], Loss: 0.1176\n",
            "Epoch [152/200], Step [300/500], Loss: 0.2871\n",
            "Epoch [152/200], Step [400/500], Loss: 0.2042\n",
            "Epoch [152/200], Step [500/500], Loss: 0.4616\n",
            "Epoch [153/200], Step [100/500], Loss: 0.1945\n",
            "Epoch [153/200], Step [200/500], Loss: 0.2535\n",
            "Epoch [153/200], Step [300/500], Loss: 0.2652\n",
            "Epoch [153/200], Step [400/500], Loss: 0.1935\n",
            "Epoch [153/200], Step [500/500], Loss: 0.1221\n",
            "Epoch [154/200], Step [100/500], Loss: 0.2702\n",
            "Epoch [154/200], Step [200/500], Loss: 0.1351\n",
            "Epoch [154/200], Step [300/500], Loss: 0.1851\n",
            "Epoch [154/200], Step [400/500], Loss: 0.1690\n",
            "Epoch [154/200], Step [500/500], Loss: 0.0996\n",
            "Epoch [155/200], Step [100/500], Loss: 0.3156\n",
            "Epoch [155/200], Step [200/500], Loss: 0.0762\n",
            "Epoch [155/200], Step [300/500], Loss: 0.2966\n",
            "Epoch [155/200], Step [400/500], Loss: 0.2442\n",
            "Epoch [155/200], Step [500/500], Loss: 0.2259\n",
            "Epoch [156/200], Step [100/500], Loss: 0.2207\n",
            "Epoch [156/200], Step [200/500], Loss: 0.1922\n",
            "Epoch [156/200], Step [300/500], Loss: 0.1882\n",
            "Epoch [156/200], Step [400/500], Loss: 0.1734\n",
            "Epoch [156/200], Step [500/500], Loss: 0.1426\n",
            "Epoch [157/200], Step [100/500], Loss: 0.1121\n",
            "Epoch [157/200], Step [200/500], Loss: 0.2120\n",
            "Epoch [157/200], Step [300/500], Loss: 0.1337\n",
            "Epoch [157/200], Step [400/500], Loss: 0.1494\n",
            "Epoch [157/200], Step [500/500], Loss: 0.3099\n",
            "Epoch [158/200], Step [100/500], Loss: 0.2913\n",
            "Epoch [158/200], Step [200/500], Loss: 0.1885\n",
            "Epoch [158/200], Step [300/500], Loss: 0.2197\n",
            "Epoch [158/200], Step [400/500], Loss: 0.3050\n",
            "Epoch [158/200], Step [500/500], Loss: 0.2423\n",
            "Epoch [159/200], Step [100/500], Loss: 0.2153\n",
            "Epoch [159/200], Step [200/500], Loss: 0.1931\n",
            "Epoch [159/200], Step [300/500], Loss: 0.1999\n",
            "Epoch [159/200], Step [400/500], Loss: 0.2060\n",
            "Epoch [159/200], Step [500/500], Loss: 0.2119\n",
            "Epoch [160/200], Step [100/500], Loss: 0.1696\n",
            "Epoch [160/200], Step [200/500], Loss: 0.1129\n",
            "Epoch [160/200], Step [300/500], Loss: 0.2064\n",
            "Epoch [160/200], Step [400/500], Loss: 0.1347\n",
            "Epoch [160/200], Step [500/500], Loss: 0.0921\n",
            "Epoch [161/200], Step [100/500], Loss: 0.0548\n",
            "Epoch [161/200], Step [200/500], Loss: 0.2221\n",
            "Epoch [161/200], Step [300/500], Loss: 0.1810\n",
            "Epoch [161/200], Step [400/500], Loss: 0.3309\n",
            "Epoch [161/200], Step [500/500], Loss: 0.3389\n",
            "Epoch [162/200], Step [100/500], Loss: 0.1907\n",
            "Epoch [162/200], Step [200/500], Loss: 0.3871\n",
            "Epoch [162/200], Step [300/500], Loss: 0.2334\n",
            "Epoch [162/200], Step [400/500], Loss: 0.2498\n",
            "Epoch [162/200], Step [500/500], Loss: 0.2210\n",
            "Epoch [163/200], Step [100/500], Loss: 0.3379\n",
            "Epoch [163/200], Step [200/500], Loss: 0.1098\n",
            "Epoch [163/200], Step [300/500], Loss: 0.1302\n",
            "Epoch [163/200], Step [400/500], Loss: 0.4925\n",
            "Epoch [163/200], Step [500/500], Loss: 0.1476\n",
            "Epoch [164/200], Step [100/500], Loss: 0.2022\n",
            "Epoch [164/200], Step [200/500], Loss: 0.1269\n",
            "Epoch [164/200], Step [300/500], Loss: 0.2093\n",
            "Epoch [164/200], Step [400/500], Loss: 0.2636\n",
            "Epoch [164/200], Step [500/500], Loss: 0.0730\n",
            "Epoch [165/200], Step [100/500], Loss: 0.1898\n",
            "Epoch [165/200], Step [200/500], Loss: 0.1344\n",
            "Epoch [165/200], Step [300/500], Loss: 0.4678\n",
            "Epoch [165/200], Step [400/500], Loss: 0.6047\n",
            "Epoch [165/200], Step [500/500], Loss: 0.2343\n",
            "Epoch [166/200], Step [100/500], Loss: 0.1631\n",
            "Epoch [166/200], Step [200/500], Loss: 0.1192\n",
            "Epoch [166/200], Step [300/500], Loss: 0.3441\n",
            "Epoch [166/200], Step [400/500], Loss: 0.1258\n",
            "Epoch [166/200], Step [500/500], Loss: 0.3638\n",
            "Epoch [167/200], Step [100/500], Loss: 0.3170\n",
            "Epoch [167/200], Step [200/500], Loss: 0.1566\n",
            "Epoch [167/200], Step [300/500], Loss: 0.2952\n",
            "Epoch [167/200], Step [400/500], Loss: 0.2036\n",
            "Epoch [167/200], Step [500/500], Loss: 0.2100\n",
            "Epoch [168/200], Step [100/500], Loss: 0.2945\n",
            "Epoch [168/200], Step [200/500], Loss: 0.2235\n",
            "Epoch [168/200], Step [300/500], Loss: 0.1526\n",
            "Epoch [168/200], Step [400/500], Loss: 0.3116\n",
            "Epoch [168/200], Step [500/500], Loss: 0.2993\n",
            "Epoch [169/200], Step [100/500], Loss: 0.1821\n",
            "Epoch [169/200], Step [200/500], Loss: 0.1510\n",
            "Epoch [169/200], Step [300/500], Loss: 0.2108\n",
            "Epoch [169/200], Step [400/500], Loss: 0.1941\n",
            "Epoch [169/200], Step [500/500], Loss: 0.1615\n",
            "Epoch [170/200], Step [100/500], Loss: 0.3306\n",
            "Epoch [170/200], Step [200/500], Loss: 0.1496\n",
            "Epoch [170/200], Step [300/500], Loss: 0.3748\n",
            "Epoch [170/200], Step [400/500], Loss: 0.2494\n",
            "Epoch [170/200], Step [500/500], Loss: 0.1601\n",
            "Epoch [171/200], Step [100/500], Loss: 0.3523\n",
            "Epoch [171/200], Step [200/500], Loss: 0.1648\n",
            "Epoch [171/200], Step [300/500], Loss: 0.0867\n",
            "Epoch [171/200], Step [400/500], Loss: 0.0866\n",
            "Epoch [171/200], Step [500/500], Loss: 0.6071\n",
            "Epoch [172/200], Step [100/500], Loss: 0.1983\n",
            "Epoch [172/200], Step [200/500], Loss: 0.2385\n",
            "Epoch [172/200], Step [300/500], Loss: 0.1091\n",
            "Epoch [172/200], Step [400/500], Loss: 0.1268\n",
            "Epoch [172/200], Step [500/500], Loss: 0.1719\n",
            "Epoch [173/200], Step [100/500], Loss: 0.2330\n",
            "Epoch [173/200], Step [200/500], Loss: 0.2088\n",
            "Epoch [173/200], Step [300/500], Loss: 0.1760\n",
            "Epoch [173/200], Step [400/500], Loss: 0.4365\n",
            "Epoch [173/200], Step [500/500], Loss: 0.1431\n",
            "Epoch [174/200], Step [100/500], Loss: 0.0963\n",
            "Epoch [174/200], Step [200/500], Loss: 0.2084\n",
            "Epoch [174/200], Step [300/500], Loss: 0.1862\n",
            "Epoch [174/200], Step [400/500], Loss: 0.2064\n",
            "Epoch [174/200], Step [500/500], Loss: 0.3202\n",
            "Epoch [175/200], Step [100/500], Loss: 0.1957\n",
            "Epoch [175/200], Step [200/500], Loss: 0.2559\n",
            "Epoch [175/200], Step [300/500], Loss: 0.4909\n",
            "Epoch [175/200], Step [400/500], Loss: 0.2206\n",
            "Epoch [175/200], Step [500/500], Loss: 0.1890\n",
            "Epoch [176/200], Step [100/500], Loss: 0.1900\n",
            "Epoch [176/200], Step [200/500], Loss: 0.2194\n",
            "Epoch [176/200], Step [300/500], Loss: 0.1883\n",
            "Epoch [176/200], Step [400/500], Loss: 0.1804\n",
            "Epoch [176/200], Step [500/500], Loss: 0.2595\n",
            "Epoch [177/200], Step [100/500], Loss: 0.2227\n",
            "Epoch [177/200], Step [200/500], Loss: 0.1407\n",
            "Epoch [177/200], Step [300/500], Loss: 0.2344\n",
            "Epoch [177/200], Step [400/500], Loss: 0.2987\n",
            "Epoch [177/200], Step [500/500], Loss: 0.2096\n",
            "Epoch [178/200], Step [100/500], Loss: 0.2146\n",
            "Epoch [178/200], Step [200/500], Loss: 0.1489\n",
            "Epoch [178/200], Step [300/500], Loss: 0.1520\n",
            "Epoch [178/200], Step [400/500], Loss: 0.2414\n",
            "Epoch [178/200], Step [500/500], Loss: 0.1373\n",
            "Epoch [179/200], Step [100/500], Loss: 0.2395\n",
            "Epoch [179/200], Step [200/500], Loss: 0.1546\n",
            "Epoch [179/200], Step [300/500], Loss: 0.2201\n",
            "Epoch [179/200], Step [400/500], Loss: 0.1939\n",
            "Epoch [179/200], Step [500/500], Loss: 0.2245\n",
            "Epoch [180/200], Step [100/500], Loss: 0.1598\n",
            "Epoch [180/200], Step [200/500], Loss: 0.0956\n",
            "Epoch [180/200], Step [300/500], Loss: 0.1508\n",
            "Epoch [180/200], Step [400/500], Loss: 0.1819\n",
            "Epoch [180/200], Step [500/500], Loss: 0.1622\n",
            "Epoch [181/200], Step [100/500], Loss: 0.1050\n",
            "Epoch [181/200], Step [200/500], Loss: 0.2582\n",
            "Epoch [181/200], Step [300/500], Loss: 0.0778\n",
            "Epoch [181/200], Step [400/500], Loss: 0.1846\n",
            "Epoch [181/200], Step [500/500], Loss: 0.1385\n",
            "Epoch [182/200], Step [100/500], Loss: 0.2677\n",
            "Epoch [182/200], Step [200/500], Loss: 0.2265\n",
            "Epoch [182/200], Step [300/500], Loss: 0.2381\n",
            "Epoch [182/200], Step [400/500], Loss: 0.2766\n",
            "Epoch [182/200], Step [500/500], Loss: 0.3743\n",
            "Epoch [183/200], Step [100/500], Loss: 0.3838\n",
            "Epoch [183/200], Step [200/500], Loss: 0.1034\n",
            "Epoch [183/200], Step [300/500], Loss: 0.2119\n",
            "Epoch [183/200], Step [400/500], Loss: 0.1810\n",
            "Epoch [183/200], Step [500/500], Loss: 0.1018\n",
            "Epoch [184/200], Step [100/500], Loss: 0.1624\n",
            "Epoch [184/200], Step [200/500], Loss: 0.1957\n",
            "Epoch [184/200], Step [300/500], Loss: 0.1731\n",
            "Epoch [184/200], Step [400/500], Loss: 0.2495\n",
            "Epoch [184/200], Step [500/500], Loss: 0.2408\n",
            "Epoch [185/200], Step [100/500], Loss: 0.1722\n",
            "Epoch [185/200], Step [200/500], Loss: 0.2611\n",
            "Epoch [185/200], Step [300/500], Loss: 0.1944\n",
            "Epoch [185/200], Step [400/500], Loss: 0.2779\n",
            "Epoch [185/200], Step [500/500], Loss: 0.2491\n",
            "Epoch [186/200], Step [100/500], Loss: 0.1959\n",
            "Epoch [186/200], Step [200/500], Loss: 0.1531\n",
            "Epoch [186/200], Step [300/500], Loss: 0.1448\n",
            "Epoch [186/200], Step [400/500], Loss: 0.1349\n",
            "Epoch [186/200], Step [500/500], Loss: 0.2245\n",
            "Epoch [187/200], Step [100/500], Loss: 0.1751\n",
            "Epoch [187/200], Step [200/500], Loss: 0.1789\n",
            "Epoch [187/200], Step [300/500], Loss: 0.1949\n",
            "Epoch [187/200], Step [400/500], Loss: 0.2691\n",
            "Epoch [187/200], Step [500/500], Loss: 0.2023\n",
            "Epoch [188/200], Step [100/500], Loss: 0.2226\n",
            "Epoch [188/200], Step [200/500], Loss: 0.1787\n",
            "Epoch [188/200], Step [300/500], Loss: 0.2783\n",
            "Epoch [188/200], Step [400/500], Loss: 0.2085\n",
            "Epoch [188/200], Step [500/500], Loss: 0.2107\n",
            "Epoch [189/200], Step [100/500], Loss: 0.1025\n",
            "Epoch [189/200], Step [200/500], Loss: 0.1093\n",
            "Epoch [189/200], Step [300/500], Loss: 0.2071\n",
            "Epoch [189/200], Step [400/500], Loss: 0.2522\n",
            "Epoch [189/200], Step [500/500], Loss: 0.1412\n",
            "Epoch [190/200], Step [100/500], Loss: 0.1840\n",
            "Epoch [190/200], Step [200/500], Loss: 0.2456\n",
            "Epoch [190/200], Step [300/500], Loss: 0.3452\n",
            "Epoch [190/200], Step [400/500], Loss: 0.2648\n",
            "Epoch [190/200], Step [500/500], Loss: 0.2005\n",
            "Epoch [191/200], Step [100/500], Loss: 0.1214\n",
            "Epoch [191/200], Step [200/500], Loss: 0.2211\n",
            "Epoch [191/200], Step [300/500], Loss: 0.2131\n",
            "Epoch [191/200], Step [400/500], Loss: 0.1846\n",
            "Epoch [191/200], Step [500/500], Loss: 0.1449\n",
            "Epoch [192/200], Step [100/500], Loss: 0.2391\n",
            "Epoch [192/200], Step [200/500], Loss: 0.2288\n",
            "Epoch [192/200], Step [300/500], Loss: 0.2666\n",
            "Epoch [192/200], Step [400/500], Loss: 0.2838\n",
            "Epoch [192/200], Step [500/500], Loss: 0.4083\n",
            "Epoch [193/200], Step [100/500], Loss: 0.2089\n",
            "Epoch [193/200], Step [200/500], Loss: 0.1789\n",
            "Epoch [193/200], Step [300/500], Loss: 0.2665\n",
            "Epoch [193/200], Step [400/500], Loss: 0.0454\n",
            "Epoch [193/200], Step [500/500], Loss: 0.1508\n",
            "Epoch [194/200], Step [100/500], Loss: 0.3625\n",
            "Epoch [194/200], Step [200/500], Loss: 0.2007\n",
            "Epoch [194/200], Step [300/500], Loss: 0.1530\n",
            "Epoch [194/200], Step [400/500], Loss: 0.2744\n",
            "Epoch [194/200], Step [500/500], Loss: 0.1024\n",
            "Epoch [195/200], Step [100/500], Loss: 0.3020\n",
            "Epoch [195/200], Step [200/500], Loss: 0.3120\n",
            "Epoch [195/200], Step [300/500], Loss: 0.1723\n",
            "Epoch [195/200], Step [400/500], Loss: 0.1804\n",
            "Epoch [195/200], Step [500/500], Loss: 0.0967\n",
            "Epoch [196/200], Step [100/500], Loss: 0.2332\n",
            "Epoch [196/200], Step [200/500], Loss: 0.2152\n",
            "Epoch [196/200], Step [300/500], Loss: 0.1705\n",
            "Epoch [196/200], Step [400/500], Loss: 0.1260\n",
            "Epoch [196/200], Step [500/500], Loss: 0.1743\n",
            "Epoch [197/200], Step [100/500], Loss: 0.2162\n",
            "Epoch [197/200], Step [200/500], Loss: 0.1642\n",
            "Epoch [197/200], Step [300/500], Loss: 0.1645\n",
            "Epoch [197/200], Step [400/500], Loss: 0.1554\n",
            "Epoch [197/200], Step [500/500], Loss: 0.2169\n",
            "Epoch [198/200], Step [100/500], Loss: 0.1451\n",
            "Epoch [198/200], Step [200/500], Loss: 0.2348\n",
            "Epoch [198/200], Step [300/500], Loss: 0.2648\n",
            "Epoch [198/200], Step [400/500], Loss: 0.2483\n",
            "Epoch [198/200], Step [500/500], Loss: 0.0904\n",
            "Epoch [199/200], Step [100/500], Loss: 0.0913\n",
            "Epoch [199/200], Step [200/500], Loss: 0.2913\n",
            "Epoch [199/200], Step [300/500], Loss: 0.2537\n",
            "Epoch [199/200], Step [400/500], Loss: 0.1813\n",
            "Epoch [199/200], Step [500/500], Loss: 0.1434\n",
            "Epoch [200/200], Step [100/500], Loss: 0.1792\n",
            "Epoch [200/200], Step [200/500], Loss: 0.2701\n",
            "Epoch [200/200], Step [300/500], Loss: 0.3663\n",
            "Epoch [200/200], Step [400/500], Loss: 0.2361\n",
            "Epoch [200/200], Step [500/500], Loss: 0.2424\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuK2u7PiXEnu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81847b23-b81c-4b21-b37b-b0598d15b5a7"
      },
      "source": [
        "\"\"\"\n",
        "Evaluate model accuracy\n",
        "\"\"\"\n",
        "model.eval() \n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    \n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item() #.item returns as python number\n",
        "\n",
        "print(\"Model achieved {}% accuracy\".format((correct/total)*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model achieved 79.97% accuracy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnlYobhEGuFt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "4d5fd405-1a76-45ce-c9c1-fe4c18f488ed"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.title(\"Loss vs Epochs\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}